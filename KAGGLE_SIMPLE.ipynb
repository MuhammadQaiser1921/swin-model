{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05020008",
   "metadata": {},
   "source": [
    "# Swin-Tiny Training on Kaggle (Extracted Frames)\n",
    "\n",
    "> This notebook trains on **extracted frame datasets** (no video decoding during training).\n",
    "\n",
    "## Required Kaggle Datasets\n",
    "\n",
    "1. **FF++ frames**:\n",
    "   `/kaggle/input/datasets/muhammadqaiser1921/faceforenscis/ffpp_binary_frames`\n",
    "   - splits: `train`, `val`, `test`\n",
    "   - labels: `0=real`, `1=fake`\n",
    "\n",
    "2. **Deepfake frames**:\n",
    "   `/kaggle/input/datasets/aryansingh16/deepfake-dataset/real_vs_fake/real-vs-fake`\n",
    "   - splits: `train`, `valid`, `test`\n",
    "   - labels: `real`, `fake`\n",
    "\n",
    "## Run Order\n",
    "\n",
    "- **Cell 1**: Clone repo + verify datasets + install requirements\n",
    "- **Cell 2**: Configure paths + train model\n",
    "- **Cell 3**: View results + download outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "GITHUB_USERNAME = \"MuhammadQaiser1921\"\n",
    "REPO_NAME = \"swin-model\"\n",
    "REPO_BRANCH = \"main\"\n",
    "GITHUB_URL = f\"https://github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
    "\n",
    "FFPP_FRAMES_ROOT = \"/kaggle/input/datasets/muhammadqaiser1921/faceforenscis/ffpp_binary_frames\"\n",
    "DEEPFAKE_FRAMES_ROOT = (\n",
    "    \"/kaggle/input/datasets/aryansingh16/deepfake-dataset/real_vs_fake/real-vs-fake\"\n",
    " )\n",
    "\n",
    "# ========== CLONE / UPDATE REPO ==========\n",
    "os.chdir('/kaggle/working')\n",
    "repo_path = os.path.join('/kaggle/working', REPO_NAME)\n",
    "\n",
    "print(f\"üìå Repository: {GITHUB_URL}\")\n",
    "print(f\"üåø Branch: {REPO_BRANCH}\")\n",
    "print(f\"üìÅ Path: {repo_path}\\n\")\n",
    "\n",
    "if os.path.exists(repo_path):\n",
    "    try:\n",
    "        os.chdir(repo_path)\n",
    "        result = subprocess.run([\"git\", \"status\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úì Using existing repo, fetching updates...\")\n",
    "            subprocess.run([\"git\", \"fetch\", \"--all\"], check=True)\n",
    "            subprocess.run([\"git\", \"checkout\", REPO_BRANCH], check=True)\n",
    "            subprocess.run([\"git\", \"pull\", \"origin\", REPO_BRANCH], check=True)\n",
    "        else:\n",
    "            os.chdir('/kaggle/working')\n",
    "            print(\"‚ö†Ô∏è Invalid repo directory, removing and re-cloning...\")\n",
    "            shutil.rmtree(repo_path, ignore_errors=True)\n",
    "            subprocess.run([\"git\", \"clone\", \"-b\", REPO_BRANCH, GITHUB_URL], check=True)\n",
    "    except Exception as e:\n",
    "        os.chdir('/kaggle/working')\n",
    "        print(f\"‚ö†Ô∏è Error: {e}. Removing and re-cloning...\")\n",
    "        shutil.rmtree(repo_path, ignore_errors=True)\n",
    "        subprocess.run([\"git\", \"clone\", \"-b\", REPO_BRANCH, GITHUB_URL], check=True)\n",
    "else:\n",
    "    print(f\"Cloning {GITHUB_URL}...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"-b\", REPO_BRANCH, GITHUB_URL], check=True)\n",
    "\n",
    "print(\"‚úÖ Repository ready!\\n\")\n",
    "\n",
    "# ========== VERIFY DATASETS ==========\n",
    "def _check_path(path, label):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ {label} found:\")\n",
    "        print(f\"   {path}\")\n",
    "        print(f\"   Top-level folders: {os.listdir(path)[:6]}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {label} not found:\")\n",
    "        print(f\"   {path}\")\n",
    "\n",
    "_check_path(FFPP_FRAMES_ROOT, \"FF++ frames\")\n",
    "_check_path(DEEPFAKE_FRAMES_ROOT, \"Deepfake frames\")\n",
    "\n",
    "# ========== OUTPUT DIRECTORIES ==========\n",
    "os.makedirs(os.path.join(repo_path, 'models', 'checkpoints'), exist_ok=True)\n",
    "os.makedirs(os.path.join(repo_path, 'models', 'weights'), exist_ok=True)\n",
    "os.makedirs(os.path.join(repo_path, 'results', 'logs'), exist_ok=True)\n",
    "\n",
    "# ========== INSTALL REQUIREMENTS ==========\n",
    "sys.path.insert(0, os.path.join(repo_path, 'src'))\n",
    "req_file = os.path.join(repo_path, 'requirements.txt')\n",
    "if os.path.exists(req_file):\n",
    "    print(\"Installing requirements...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", req_file], check=True)\n",
    "    print(\"‚úÖ Requirements installed\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è requirements.txt not found\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ee969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from train_video import train_video_model, Config\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ TRAINING SWIN-TINY (EXTRACTED FRAMES)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# GPU check\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"\\nüíª GPU available: {len(gpus)} GPU(s)\")\n",
    "for gpu in gpus:\n",
    "    print(f\"   ‚úì {gpu}\")\n",
    "\n",
    "# Configure dataset roots\n",
    "Config.FFPP_FRAMES_ROOT = \"/kaggle/input/datasets/muhammadqaiser1921/faceforenscis/ffpp_binary_frames\"\n",
    "Config.DEEPFAKE_FRAMES_ROOT = (\n",
    "    \"/kaggle/input/datasets/aryansingh16/deepfake-dataset/real_vs_fake/real-vs-fake\"\n",
    " )\n",
    "Config.KAGGLE_ENV = True\n",
    "\n",
    "# Training parameters\n",
    "Config.BATCH_SIZE = 12\n",
    "Config.EPOCHS = 20\n",
    "Config.MAX_IMAGES_PER_CLASS = None  # Set to 500 for quick test\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Configuration:\")\n",
    "print(f\"   FF++ root: {Config.FFPP_FRAMES_ROOT}\")\n",
    "print(f\"   Deepfake root: {Config.DEEPFAKE_FRAMES_ROOT}\")\n",
    "print(f\"   Batch size: {Config.BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {Config.EPOCHS}\")\n",
    "print(f\"   Max images/class: {Config.MAX_IMAGES_PER_CLASS}\")\n",
    "\n",
    "print(\"\\nüéØ Starting training...\\n\")\n",
    "model, history, auc_metrics = train_video_model()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TRAINING COMPLETED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if auc_metrics:\n",
    "    print(\"\\nüìä AUC METRICS:\")\n",
    "    print(f\"   AUC-ROC: {auc_metrics.get('auc_roc', 'N/A'):.4f}\")\n",
    "    print(f\"   PR-AUC: {auc_metrics.get('pr_auc', 'N/A'):.4f}\")\n",
    "    print(f\"   Optimal Threshold: {auc_metrics.get('optimal_threshold', 'N/A'):.4f}\")\n",
    "    print(f\"   Sensitivity: {auc_metrics.get('optimal_sensitivity', 'N/A'):.4f}\")\n",
    "    print(f\"   Specificity: {auc_metrics.get('optimal_specificity', 'N/A'):.4f}\")\n",
    "    print(f\"   F1-Score: {auc_metrics.get('optimal_f1', 'N/A'):.4f}\")\n",
    "\n",
    "print(\"\\nüíæ Outputs:\")\n",
    "print(f\"   ‚Ä¢ {Config.CHECKPOINT_DIR}\")\n",
    "print(f\"   ‚Ä¢ {Config.WEIGHTS_DIR}\")\n",
    "print(f\"   ‚Ä¢ {Config.LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8afa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üì• RESULTS & DOWNLOAD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_dir = Config.LOG_DIR\n",
    "weights_dir = Config.WEIGHTS_DIR\n",
    "checkpoints_dir = Config.CHECKPOINT_DIR\n",
    "\n",
    "print(f\"\\nLogs: {results_dir}\")\n",
    "print(f\"Weights: {weights_dir}\")\n",
    "print(f\"Checkpoints: {checkpoints_dir}\")\n",
    "\n",
    "# AUC metrics\n",
    "auc_files = sorted(glob.glob(os.path.join(results_dir, 'auc_metrics_*.json')))\n",
    "if auc_files:\n",
    "    latest = auc_files[-1]\n",
    "    print(f\"\\n‚úÖ AUC metrics: {os.path.basename(latest)}\")\n",
    "    with open(latest, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "        print(f\"   AUC-ROC: {metrics.get('auc_roc', 'N/A'):.4f}\")\n",
    "        print(f\"   PR-AUC: {metrics.get('pr_auc', 'N/A'):.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No AUC metrics found\")\n",
    "\n",
    "# Weights\n",
    "weight_files = sorted(glob.glob(os.path.join(weights_dir, '*weights*.h5')))\n",
    "if weight_files:\n",
    "    print(\"\\n‚úÖ Weight files:\")\n",
    "    for path in weight_files[-3:]:\n",
    "        size_mb = os.path.getsize(path) / (1024**2)\n",
    "        print(f\"   {os.path.basename(path)} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No weights found\")\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_files = sorted(glob.glob(os.path.join(checkpoints_dir, '*.h5')))\n",
    "if checkpoint_files:\n",
    "    print(\"\\n‚úÖ Checkpoints:\")\n",
    "    for path in checkpoint_files[-2:]:\n",
    "        size_mb = os.path.getsize(path) / (1024**2)\n",
    "        print(f\"   {os.path.basename(path)} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No checkpoints found\")\n",
    "\n",
    "print(\"\\nüì• Download: Kaggle ‚Üí Output ‚Üí Download all\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
