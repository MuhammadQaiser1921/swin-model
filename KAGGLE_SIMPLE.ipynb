{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05020008",
   "metadata": {},
   "source": [
    "# Swin-Tiny Training on Kaggle (Extracted Frames)\n",
    "\n",
    "> This notebook trains on **extracted frame datasets** (no video decoding during training).\n",
    "\n",
    "## Required Kaggle Datasets\n",
    "\n",
    "1. **FF++ frames**:\n",
    "   `/kaggle/input/datasets/muhammadqaiser1921/faceforenscis/ffpp_binary_frames`\n",
    "   - splits: `train`, `val`, `test`\n",
    "   - labels: `0=real`, `1=fake`\n",
    "\n",
    "2. **Deepfake frames**:\n",
    "   `/kaggle/input/datasets/aryansingh16/deepfake-dataset/real_vs_fake/real-vs-fake`\n",
    "   - splits: `train`, `valid`, `test`\n",
    "   - labels: `real`, `fake`\n",
    "\n",
    "## Run Order\n",
    "\n",
    "- **Cell 1**: Clone repo + verify datasets + install requirements\n",
    "- **Cell 2**: Load data (RUN ONCE - no need to re-run when updating model)\n",
    "- **Cell 3**: Build and train model (RE-RUN as needed after editing model code)\n",
    "- **Cell 4**: View results + download outputs\n",
    "\n",
    "## Iterative Development Workflow\n",
    "\n",
    "After running cells 1-3 once, you can:\n",
    "1. Edit `swin_transformer.py` in your repo\n",
    "2. Commit and push changes\n",
    "3. Re-run Cell 1 to pull updates\n",
    "4. Re-run Cell 3 only (skips data loading!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "GITHUB_USERNAME = \"MuhammadQaiser1921\"\n",
    "REPO_NAME = \"swin-model\"\n",
    "REPO_BRANCH = \"main\"\n",
    "GITHUB_URL = \"https://github.com/MuhammadQaiser1921/swin-model.git\"\n",
    "\n",
    "FFPP_FRAMES_ROOT = \"/kaggle/input/datasets/muhammadqaiser1921/faceforenscis/ffpp_binary_frames\"\n",
    "DEEPFAKE_FRAMES_ROOT = (\n",
    "    \"/kaggle/input/datasets/aryansingh16/deepfake-dataset/real_vs_fake/real-vs-fake\"\n",
    " )\n",
    "\n",
    "# ========== CLONE / UPDATE REPO ==========\n",
    "os.chdir('/kaggle/working')\n",
    "repo_path = os.path.join('/kaggle/working', REPO_NAME)\n",
    "\n",
    "print(f\"üìå Repository: {GITHUB_URL}\")\n",
    "print(f\"üåø Branch: {REPO_BRANCH}\")\n",
    "print(f\"üìÅ Path: {repo_path}\\n\")\n",
    "\n",
    "if os.path.exists(repo_path):\n",
    "    try:\n",
    "        os.chdir(repo_path)\n",
    "        result = subprocess.run([\"git\", \"status\"], capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úì Using existing repo, fetching updates...\")\n",
    "            subprocess.run([\"git\", \"fetch\", \"--all\"], check=True)\n",
    "            subprocess.run([\"git\", \"checkout\", REPO_BRANCH], check=True)\n",
    "            subprocess.run([\"git\", \"pull\", \"origin\", REPO_BRANCH], check=True)\n",
    "        else:\n",
    "            os.chdir('/kaggle/working')\n",
    "            print(\"‚ö†Ô∏è Invalid repo directory, removing and re-cloning...\")\n",
    "            shutil.rmtree(repo_path, ignore_errors=True)\n",
    "            subprocess.run([\"git\", \"clone\", \"-b\", REPO_BRANCH, GITHUB_URL], check=True)\n",
    "    except Exception as e:\n",
    "        os.chdir('/kaggle/working')\n",
    "        print(f\"‚ö†Ô∏è Error: {e}. Removing and re-cloning...\")\n",
    "        shutil.rmtree(repo_path, ignore_errors=True)\n",
    "        subprocess.run([\"git\", \"clone\", \"-b\", REPO_BRANCH, GITHUB_URL], check=True)\n",
    "else:\n",
    "    print(f\"Cloning {GITHUB_URL}...\")\n",
    "    subprocess.run([\"git\", \"clone\", \"-b\", REPO_BRANCH, GITHUB_URL], check=True)\n",
    "\n",
    "print(\"‚úÖ Repository ready!\\n\")\n",
    "\n",
    "# ========== VERIFY DATASETS ==========\n",
    "def _check_path(path, label):\n",
    "    if os.path.exists(path):\n",
    "        print(f\"‚úÖ {label} found:\")\n",
    "        print(f\"   {path}\")\n",
    "        print(f\"   Top-level folders: {os.listdir(path)[:6]}\")\n",
    "    else:\n",
    "        print(f\"‚ùå {label} not found:\")\n",
    "        print(f\"   {path}\")\n",
    "\n",
    "_check_path(FFPP_FRAMES_ROOT, \"FF++ frames\")\n",
    "_check_path(DEEPFAKE_FRAMES_ROOT, \"Deepfake frames\")\n",
    "\n",
    "# ========== OUTPUT DIRECTORIES ==========\n",
    "os.makedirs(os.path.join(repo_path, 'models', 'checkpoints'), exist_ok=True)\n",
    "os.makedirs(os.path.join(repo_path, 'models', 'weights'), exist_ok=True)\n",
    "os.makedirs(os.path.join(repo_path, 'results', 'logs'), exist_ok=True)\n",
    "\n",
    "# ========== INSTALL REQUIREMENTS ==========\n",
    "sys.path.insert(0, os.path.join(repo_path, 'src'))\n",
    "req_file = os.path.join(repo_path, 'requirements.txt')\n",
    "if os.path.exists(req_file):\n",
    "    print(\"Installing requirements...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", req_file], check=True)\n",
    "    print(\"‚úÖ Requirements installed\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è requirements.txt not found\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ee969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from video_train_config import Config\n",
    "from data_loader import load_data, prepare_datasets\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ STEP 1: LOAD DATA (RUN ONCE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# GPU check\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"\\nüíª GPU available: {len(gpus)} GPU(s)\")\n",
    "for gpu in gpus:\n",
    "    print(f\"   ‚úì {gpu}\")\n",
    "\n",
    "# Configure dataset roots\n",
    "Config.FFPP_FRAMES_ROOT = \"/kaggle/input/datasets/muhammadqaiser1921/faceforenscis/ffpp_binary_frames\"\n",
    "Config.DEEPFAKE_FRAMES_ROOT = (\n",
    "    \"/kaggle/input/datasets/aryansingh16/deepfake-dataset/real_vs_fake/real-vs-fake\"\n",
    ")\n",
    "Config.KAGGLE_ENV = True\n",
    "\n",
    "# Training parameters\n",
    "Config.BATCH_SIZE = 12\n",
    "Config.EPOCHS = 20\n",
    "Config.MAX_IMAGES_PER_CLASS = None  # Set to 500 for quick test\n",
    "\n",
    "print(\"\\n‚öôÔ∏è Configuration:\")\n",
    "print(f\"   FF++ root: {Config.FFPP_FRAMES_ROOT}\")\n",
    "print(f\"   Deepfake root: {Config.DEEPFAKE_FRAMES_ROOT}\")\n",
    "print(f\"   Batch size: {Config.BATCH_SIZE}\")\n",
    "print(f\"   Epochs: {Config.EPOCHS}\")\n",
    "print(f\"   Max images/class: {Config.MAX_IMAGES_PER_CLASS}\")\n",
    "\n",
    "# Load data\n",
    "print(\"\\nüìÇ Loading data...\")\n",
    "data = load_data()\n",
    "\n",
    "# Prepare datasets\n",
    "train_ds, val_ds, test_ds = prepare_datasets(data)\n",
    "\n",
    "print(\"\\n‚úÖ Data loading complete. You can now re-run Cell 3 to train different models without reloading data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6212a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_video import build_and_compile_model, train_model\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üöÄ STEP 2: BUILD AND TRAIN MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Build and compile the model\n",
    "model = build_and_compile_model()\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nüéØ Starting training...\\n\")\n",
    "history = train_model(model, train_ds, val_ds)\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(f\"\\nüìä Computing metrics on validation set...\")\n",
    "from train_video import compute_auc_metrics\n",
    "import numpy as np\n",
    "\n",
    "y_val_pred_probs = model.predict(val_ds)\n",
    "y_val_pred_probs = y_val_pred_probs[:, 1]  # Get probabilities for class 1 (fake)\n",
    "\n",
    "auc_metrics = compute_auc_metrics(data['val_labels'], y_val_pred_probs)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ TRAINING COMPLETED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìà Training Performance:\")\n",
    "print(f\"   Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"   Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"   Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "\n",
    "if auc_metrics:\n",
    "    print(\"\\nüìä Validation AUC Metrics:\")\n",
    "    print(f\"   AUC-ROC: {auc_metrics.get('auc_roc', 'N/A'):.4f}\")\n",
    "    print(f\"   PR-AUC: {auc_metrics.get('pr_auc', 'N/A'):.4f}\")\n",
    "    print(f\"   Optimal Threshold: {auc_metrics.get('optimal_threshold', 'N/A'):.4f}\")\n",
    "    print(f\"   Sensitivity: {auc_metrics.get('optimal_sensitivity', 'N/A'):.4f}\")\n",
    "    print(f\"   Specificity: {auc_metrics.get('optimal_specificity', 'N/A'):.4f}\")\n",
    "    print(f\"   F1-Score: {auc_metrics.get('optimal_f1', 'N/A'):.4f}\")\n",
    "\n",
    "# Test set evaluation (if available)\n",
    "if test_ds is not None and len(data['test_paths']) > 0:\n",
    "    print(\"\\nüìä Computing test set metrics...\")\n",
    "    y_test_pred_probs = model.predict(test_ds)\n",
    "    y_test_pred_probs = y_test_pred_probs[:, 1]\n",
    "    test_auc_metrics = compute_auc_metrics(data['test_labels'], y_test_pred_probs)\n",
    "    print(f\"   Test AUC-ROC: {test_auc_metrics.get('auc_roc', 'N/A'):.4f}\")\n",
    "    print(f\"   Test PR-AUC: {test_auc_metrics.get('pr_auc', 'N/A'):.4f}\")\n",
    "\n",
    "# Save model\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "final_model_path = os.path.join(\n",
    "    Config.CHECKPOINT_DIR,\n",
    "    f\"swin_tiny_final_{datetime.now().strftime('%Y%m%d_%H%M%S')}.h5\"\n",
    ")\n",
    "model.save(final_model_path)\n",
    "print(f\"\\nüíæ Model saved to: {final_model_path}\")\n",
    "\n",
    "print(\"\\nüíæ Output directories:\")\n",
    "print(f\"   ‚Ä¢ {Config.CHECKPOINT_DIR}\")\n",
    "print(f\"   ‚Ä¢ {Config.WEIGHTS_DIR}\")\n",
    "print(f\"   ‚Ä¢ {Config.LOG_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8afa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üì• RESULTS & DOWNLOAD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_dir = Config.LOG_DIR\n",
    "weights_dir = Config.WEIGHTS_DIR\n",
    "checkpoints_dir = Config.CHECKPOINT_DIR\n",
    "\n",
    "print(f\"\\nLogs: {results_dir}\")\n",
    "print(f\"Weights: {weights_dir}\")\n",
    "print(f\"Checkpoints: {checkpoints_dir}\")\n",
    "\n",
    "# AUC metrics\n",
    "auc_files = sorted(glob.glob(os.path.join(results_dir, 'auc_metrics_*.json')))\n",
    "if auc_files:\n",
    "    latest = auc_files[-1]\n",
    "    print(f\"\\n‚úÖ AUC metrics: {os.path.basename(latest)}\")\n",
    "    with open(latest, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "        print(f\"   AUC-ROC: {metrics.get('auc_roc', 'N/A'):.4f}\")\n",
    "        print(f\"   PR-AUC: {metrics.get('pr_auc', 'N/A'):.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No AUC metrics found\")\n",
    "\n",
    "# Weights\n",
    "weight_files = sorted(glob.glob(os.path.join(weights_dir, '*weights*.h5')))\n",
    "if weight_files:\n",
    "    print(\"\\n‚úÖ Weight files:\")\n",
    "    for path in weight_files[-3:]:\n",
    "        size_mb = os.path.getsize(path) / (1024**2)\n",
    "        print(f\"   {os.path.basename(path)} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No weights found\")\n",
    "\n",
    "# Checkpoints\n",
    "checkpoint_files = sorted(glob.glob(os.path.join(checkpoints_dir, '*.h5')))\n",
    "if checkpoint_files:\n",
    "    print(\"\\n‚úÖ Checkpoints:\")\n",
    "    for path in checkpoint_files[-2:]:\n",
    "        size_mb = os.path.getsize(path) / (1024**2)\n",
    "        print(f\"   {os.path.basename(path)} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No checkpoints found\")\n",
    "\n",
    "print(\"\\nüì• Download: Kaggle ‚Üí Output ‚Üí Download all\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
